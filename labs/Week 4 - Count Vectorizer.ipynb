{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountVectorizer:\n",
    "    def __init__(self, ngram_size):\n",
    "        \"\"\"\n",
    "        Constructor for CountVectorizer\n",
    "\n",
    "        :param ngram_size: size of tokens\n",
    "\n",
    "        :returns: None\n",
    "        \"\"\"\n",
    "        self.ngram_size = ngram_size\n",
    "\n",
    "    def tokenise(string : str, ngram_size : int):\n",
    "        \"\"\"\n",
    "        This method tokenises a string into a list of tokens. Since there's a need to tokenize inputs both in 'fit' and 'transform' methods, additional method reduces repetativity\n",
    "\n",
    "        :param string: string to be tokenised\n",
    "        :param ngram_size: size of tokens\n",
    "\n",
    "        :yields: list of tokens\n",
    "        \"\"\"\n",
    "        for i in range(len(string) - ngram_size + 1):\n",
    "            yield string[i:i + ngram_size]\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        \"\"\"\n",
    "        This method encodes sorted tokens into indexes and returns a respective dict\n",
    "\n",
    "        :param corpus: list of strings\n",
    "\n",
    "        :returns: token_to_index, a dictionary with tokens as keys, indexes as values\n",
    "        \"\"\"\n",
    "        self.token_to_index = dict()  #initializing a dict\n",
    "\n",
    "        #iterating over the corpus based on ngram_size param\n",
    "        for string in corpus:\n",
    "            for token in CountVectorizer.tokenise(string, self.ngram_size):\n",
    "                if token not in self.token_to_index:\n",
    "                    self.token_to_index[token] = len(self.token_to_index)        \n",
    "        #sorting in lexicographic order by key\n",
    "        return {token: index for index, token in enumerate(sorted(self.token_to_index.keys()))}\n",
    "\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        \"\"\"\n",
    "        This method encodes corpus into a list of lists with counts of tokens in each string/line/etc\n",
    "\n",
    "        :param corpus: list of strings\n",
    "\n",
    "        :returns: index_to_count, a list of lists with counts of tokens in each string\n",
    "        \"\"\"\n",
    "        self.index_to_count = []\n",
    "        # iterating over the corpus\n",
    "        for string in corpus:\n",
    "            str_dict = {token:0 for token in self.token_to_index.keys()}  #\n",
    "            # iterating over the tokens in the string\n",
    "            for token in CountVectorizer.tokenise(string, self.ngram_size):\n",
    "                if token in self.token_to_index:\n",
    "                    str_dict[token] += 1  # incrementing the index's vector value \n",
    "            self.index_to_count.append([str_dict[token] for token in sorted(str_dict.keys())])\n",
    "        return self.index_to_count\n",
    "        \n",
    "    def fit_transform(self, corpus):\n",
    "        \"\"\"\n",
    "        This method simultaneously calls fit and transform on the corpus.\n",
    "\n",
    "        :param corpus: list of strings\n",
    "\n",
    "        :returns: index_to_count, a list of lists with counts of tokens in each string\n",
    "        \"\"\"\n",
    "        self.fit(corpus)\n",
    "        return self.transform(corpus)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
